name: DevSecOps Pipeline

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  schedule:
    - cron: '30 1 * * 0'

permissions:
  contents: read
  packages: write
  security-events: write

jobs:
  tms-scan:
    name: 2MS Secret Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Run 2MS and save output
        continue-on-error: true
        run: |
          mkdir -p results
          chmod 777 results
          docker run --rm -v $(pwd):/repo checkmarx/2ms:2.8.1 git /repo --stdout-format json > results/2ms-raw.txt || true
          grep -A 999999 '^{' results/2ms-raw.txt > results/2ms-clean.json || echo '{"results":{}}' > results/2ms-clean.json
      
      - name: Convert to SARIF
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          
          sarif = {
              "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
              "version": "2.1.0",
              "runs": [{
                  "tool": {
                      "driver": {
                          "name": "2MS",
                          "informationUri": "https://checkmarx.com/product/2ms/",
                          "version": "2.8.1",
                          "rules": []
                      }
                  },
                  "results": []
              }]
          }
          
          try:
              with open('results/2ms-clean.json', 'r') as f:
                  data = json.load(f)
                  results = data.get('results', {})
                  
                  for commit_hash, findings in results.items():
                      for finding in findings:
                          rule_id = finding.get('ruleId', 'secret-detected')
                          source = finding.get('source', 'unknown')
                          start_line = finding.get('startLine', 1)
                          file_path = source.split(':')[-1] if ':' in source else 'unknown'
                          
                          if not any(r['id'] == rule_id for r in sarif['runs'][0]['tool']['driver']['rules']):
                              sarif['runs'][0]['tool']['driver']['rules'].append({
                                  "id": rule_id,
                                  "name": rule_id,
                                  "shortDescription": {"text": f"Detected {rule_id}"},
                                  "defaultConfiguration": {"level": "warning"}
                              })
                          
                          sarif['runs'][0]['results'].append({
                              "ruleId": rule_id,
                              "message": {"text": f"Found {rule_id} in {file_path}"},
                              "locations": [{
                                  "physicalLocation": {
                                      "artifactLocation": {"uri": file_path},
                                      "region": {"startLine": max(1, start_line)}
                                  }
                              }],
                              "level": "warning"
                          })
                  
                  print(f"Converted {len(sarif['runs'][0]['results'])} findings to SARIF")
          except Exception as e:
              print(f"Error: {e}")
          
          with open('2ms-results.sarif', 'w') as f:
              json.dump(sarif, f, indent=2)
          PYTHON_SCRIPT
      
      - name: Upload 2MS SARIF to Code Scanning
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 2ms-results.sarif
          category: 2ms
      
      - name: Save 2MS SARIF as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 2ms-sarif
          path: 2ms-results.sarif

  build-and-push:
    name: Build and Push to GHCR
    runs-on: ubuntu-latest
    needs: tms-scan
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:latest
            ghcr.io/${{ github.repository }}:${{ github.sha }}

  semgrep-scan:
    name: Semgrep SAST Scan
    runs-on: ubuntu-latest
    needs: tms-scan
    permissions:
      contents: read
      security-events: write
      actions: read
    container:
      image: returntocorp/semgrep
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Run Semgrep
        run: semgrep scan --sarif --output semgrep.sarif
      
      - name: Upload SARIF to Code Scanning
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: semgrep.sarif
      
      - name: Save Semgrep SARIF as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: semgrep-sarif
          path: semgrep.sarif

  trivy-scan:
    name: Trivy Container Scan
    runs-on: ubuntu-latest
    needs: build-and-push
    permissions:
      contents: read
      security-events: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Run Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'ghcr.io/${{ github.repository }}:latest'
          format: 'sarif'
          output: 'trivy.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '0'
      
      - name: Upload SARIF to Code Scanning
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy.sarif'
      
      - name: Save Trivy SARIF as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trivy-sarif
          path: trivy.sarif

  codeql:
    name: CodeQL SAST Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: 'javascript'
          queries: security-extended
      
      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

  ai-comprehensive-analysis:
    name: AI Security Report
    runs-on: ubuntu-latest
    needs: [semgrep-scan, trivy-scan, codeql, tms-scan]
    if: always()
    permissions:
      contents: read
      security-events: write
      pull-requests: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Download all SARIF results
        uses: actions/download-artifact@v4
        with:
          path: sarif-results
      
      - name: Install AI Analysis Tools
        run: pip install anthropic
      
      - name: Generate Comprehensive Security Report
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python3 << 'PYEOF'
          import anthropic
          import os
          import json
          import glob
          from collections import defaultdict
          
          api_key = os.environ.get("ANTHROPIC_API_KEY")
          if not api_key:
              print("WARNING: ANTHROPIC_API_KEY not set. Skipping AI analysis.")
              report = """# AI-Powered Security Analysis Report
          
          ## Setup Required
          
          To enable AI-powered analysis, add ANTHROPIC_API_KEY to GitHub Secrets.
          """
              with open('AI_SECURITY_REPORT.md', 'w') as f:
                  f.write(report)
              exit(0)
          
          client = anthropic.Anthropic(api_key=api_key)
          
          all_findings = defaultdict(list)
          severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
          
          for sarif_file in glob.glob("sarif-results/**/*.sarif", recursive=True):
              try:
                  with open(sarif_file, 'r') as f:
                      data = json.load(f)
                      tool = data['runs'][0]['tool']['driver']['name']
                      results = data['runs'][0].get('results', [])
                      
                      for result in results:
                          rule_id = result.get('ruleId', 'unknown')
                          message = result.get('message', {}).get('text', '')
                          level = result.get('level', 'warning')
                          
                          location = result.get('locations', [{}])[0]
                          file_path = location.get('physicalLocation', {}).get('artifactLocation', {}).get('uri', 'unknown')
                          
                          all_findings[tool].append({
                              'rule': rule_id,
                              'message': message,
                              'severity': level,
                              'file': file_path
                          })
                          
                          if 'error' in level or 'critical' in level.lower():
                              severity_counts['critical'] += 1
                          elif 'warning' in level or 'high' in level.lower():
                              severity_counts['high'] += 1
                          else:
                              severity_counts['medium'] += 1
              except Exception as e:
                  print(f"Error processing {sarif_file}: {e}")
          
          tools_summary = json.dumps({tool: len(findings) for tool, findings in all_findings.items()}, indent=2)
          sample_findings = json.dumps(list(all_findings.values())[0][:5] if all_findings else [], indent=2)
          
          summary = f"""Security Scan Summary
          
          Total Findings by Tool:
          {tools_summary}
          
          Severity Distribution:
          - Critical: {severity_counts['critical']}
          - High: {severity_counts['high']}
          - Medium: {severity_counts['medium']}
          - Low: {severity_counts['low']}
          
          Sample Findings (first 5):
          {sample_findings}
          """
          
          prompt = f"""{summary}

As an expert security analyst, provide:
1. Executive Summary (2-3 sentences on overall security posture)
2. Priority Fixes (Top 5 issues to address immediately with clear reasoning)
3. Risk Score (1-10 scale with justification)
4. False Positive Assessment (Identify potential false positives)
5. Remediation Roadmap (30/60/90 day plan)
6. Best Practices (3 recommendations for this codebase)

Be specific, actionable, and interview-ready."""
          
          message = client.messages.create(
              model="claude-3-7-sonnet-20250219",
              max_tokens=3000,
              messages=[{"role": "user", "content": prompt}]
          )
          
          analysis = message.content[0].text
          
          tools_list = '\n'.join([f"- **{tool}**: {len(findings)} findings" for tool, findings in all_findings.items()])
          
          report = f"""# AI-Powered Security Analysis Report
          
{analysis}

---

## Detailed Findings

**Total Issues:** {sum(len(f) for f in all_findings.values())}

### By Tool:
{tools_list}

### Severity Breakdown:
- Critical: {severity_counts['critical']}
- High: {severity_counts['high']}
- Medium: {severity_counts['medium']}
- Low: {severity_counts['low']}

---
*Report generated by Claude 3.7 Sonnet*
"""
          
          with open('AI_SECURITY_REPORT.md', 'w') as f:
              f.write(report)
          
          print(report)
          PYEOF
      
      - name: Upload Comprehensive Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-security-report
          path: AI_SECURITY_REPORT.md

  ai-pr-review:
    name: AI PR Security Review
    runs-on: ubuntu-latest
    needs: [semgrep-scan, trivy-scan, codeql, tms-scan]
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write
      security-events: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Download SARIF results
        uses: actions/download-artifact@v4
        with:
          path: sarif-results
      
      - name: Install dependencies
        run: pip install anthropic
      
      - name: Generate PR Security Review
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python3 << 'PYEOF'
          import anthropic
          import os
          import json
          import glob
          from collections import defaultdict
          
          api_key = os.environ.get("ANTHROPIC_API_KEY")
          if not api_key:
              print("No API key, skipping PR review")
              exit(0)
          
          client = anthropic.Anthropic(api_key=api_key)
          
          critical_alerts = []
          high_alerts = []
          medium_alerts = []
          all_tools = set()
          
          for sarif_file in glob.glob("sarif-results/**/*.sarif", recursive=True):
              try:
                  with open(sarif_file, 'r') as f:
                      data = json.load(f)
                      tool = data['runs'][0]['tool']['driver']['name']
                      all_tools.add(tool)
                      results = data['runs'][0].get('results', [])
                      
                      for result in results[:30]:
                          rule_id = result.get('ruleId', 'unknown')
                          message = result.get('message', {}).get('text', '')
                          level = result.get('level', 'warning')
                          
                          location = result.get('locations', [{}])[0]
                          phys_loc = location.get('physicalLocation', {})
                          file_path = phys_loc.get('artifactLocation', {}).get('uri', 'unknown')
                          line = phys_loc.get('region', {}).get('startLine', 0)
                          
                          alert_info = {
                              "tool": tool,
                              "rule": rule_id,
                              "message": message[:100],
                              "file": f"{file_path}:{line}"
                          }
                          
                          if 'error' in level or 'critical' in level.lower():
                              critical_alerts.append(alert_info)
                          elif 'warning' in level or 'high' in level.lower():
                              high_alerts.append(alert_info)
                          else:
                              medium_alerts.append(alert_info)
              except Exception as e:
                  print(f"Error processing {sarif_file}: {e}")
          
          critical_json = json.dumps(critical_alerts[:5], indent=2)
          high_json = json.dumps(high_alerts[:5], indent=2)
          tools_list = ', '.join(all_tools)
          
          summary = f"""Security Scan Results for this PR
          
Critical Issues: {len(critical_alerts)}
{critical_json}

High Severity Issues: {len(high_alerts)}
{high_json}

Medium Severity Issues: {len(medium_alerts)}

Total alerts: {len(critical_alerts) + len(high_alerts) + len(medium_alerts)}
Tools used: {tools_list}"""
          
          prompt = f"""{summary}

As a senior security engineer reviewing this PR, provide:

1. Executive Summary (2 sentences - should this PR be merged?)
2. Must-Fix Before Merge (Top 3 critical issues with specific file:line, or None if safe)
3. Recommended Fixes (Top 3 high-priority issues with remediation)
4. False Positive Assessment (Which alerts can be safely ignored and why)
5. Merge Recommendation (Approve / Approve with changes / Request changes)

Be concise, specific, and reference exact files/lines."""
          
          message = client.messages.create(
              model="claude-3-7-sonnet-20250219",
              max_tokens=2000,
              messages=[{"role": "user", "content": prompt}]
          )
          
          analysis = message.content[0].text
          
          pr_comment = f"""## AI Security Review

{analysis}

---

### Scan Summary
- Critical Issues: {len(critical_alerts)}
- High Severity: {len(high_alerts)}
- Medium Severity: {len(medium_alerts)}
- Total Alerts: {len(critical_alerts) + len(high_alerts) + len(medium_alerts)}

### Security Tools Used
- CodeQL - Deep SAST analysis
- Semgrep - Pattern-based SAST
- Trivy - Container vulnerability scanning
- 2MS - Secret detection

---
*AI-powered security analysis by Claude 3.7 Sonnet*
*This is an automated review. Please verify all findings manually.*
"""
          
          with open('pr_security_review.md', 'w') as f:
              f.write(pr_comment)
          
          print(pr_comment)
          PYEOF
      
      - name: Post PR Review Comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('pr_security_review.md', 'utf8');
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            
            const botComment = comments.find(c => c.body.includes('AI Security Review'));
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

  ai-inline-review:
    name: AI Inline Code Comments
    runs-on: ubuntu-latest
    needs: [semgrep-scan, trivy-scan, codeql, tms-scan]
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Download SARIF files
        uses: actions/download-artifact@v4
        with:
          path: sarif-results
      
      - name: Install dependencies
        run: pip install anthropic
      
      - name: Create Inline PR Comments
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python3 << 'PYEOF'
          import anthropic
          import os
          import json
          import glob
          from time import sleep
          
          api_key = os.environ.get("ANTHROPIC_API_KEY")
          if not api_key:
              print("No API key, skipping inline comments")
              with open('review_comments.json', 'w') as f:
                  json.dump([], f)
              exit(0)
          
          client = anthropic.Anthropic(api_key=api_key)
          
          review_comments = []
          processed = 0
          max_comments = 15
          
          for sarif_file in glob.glob("sarif-results/**/*.sarif", recursive=True):
              if processed >= max_comments:
                  break
                  
              try:
                  with open(sarif_file) as f:
                      data = json.load(f)
                      tool = data['runs'][0]['tool']['driver']['name']
                      
                      for result in data['runs'][0].get('results', []):
                          if processed >= max_comments:
                              break
                          
                          rule_id = result.get('ruleId', 'unknown')
                          message = result.get('message', {}).get('text', '')
                          level = result.get('level', 'warning')
                          
                          location = result.get('locations', [{}])[0]
                          phys_loc = location.get('physicalLocation', {})
                          file_path = phys_loc.get('artifactLocation', {}).get('uri', '')
                          line = phys_loc.get('region', {}).get('startLine', 1)
                          
                          if not file_path or line < 1:
                              continue
                          
                          if level not in ['error', 'warning']:
                              continue
                          
                          prompt = f"""Security alert:
Tool: {tool}
Rule: {rule_id}
Issue: {message}

In 1-2 sentences: Is this real or false positive? If real, how to fix it quickly?"""
                          
                          try:
                              ai_msg = client.messages.create(
                                  model="claude-3-7-sonnet-20250219",
                                  max_tokens=200,
                                  messages=[{"role": "user", "content": prompt}]
                              )
                              
                              analysis = ai_msg.content[0].text
                              
                              severity_emoji = "ðŸ”´" if level == "error" else "ðŸŸ "
                              
                              review_comments.append({
                                  "path": file_path,
                                  "line": line,
                                  "body": f"{severity_emoji} **{tool}** - `{rule_id}`\n\n{analysis}\n\n*AI-assisted analysis*"
                              })
                              
                              processed += 1
                              print(f"Generated comment {processed}/{max_comments} for {file_path}:{line}")
                              
                              sleep(1.5)
                              
                          except Exception as e:
                              print(f"Error analyzing: {e}")
                              continue
                              
              except Exception as e:
                  print(f"Error processing {sarif_file}: {e}")
          
          with open('review_comments.json', 'w') as f:
              json.dump(review_comments, f)
          
          print(f"Created {len(review_comments)} inline comments")
          PYEOF
      
      - name: Post Inline Comments
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comments = JSON.parse(fs.readFileSync('review_comments.json', 'utf8'));
            
            console.log(`Posting ${comments.length} inline comments`);
            
            for (const comment of comments) {
              try {
                await github.rest.pulls.createReviewComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  pull_number: context.issue.number,
                  body: comment.body,
                  path: comment.path,
                  line: comment.line,
                  commit_id: context.payload.pull_request.head.sha
                });
                console.log(`âœ“ Commented on ${comment.path}:${comment.line}`);
              } catch (error) {
                console.log(`âœ— Failed to comment on ${comment.path}:${comment.line}: ${error.message}`);
              }
            }
